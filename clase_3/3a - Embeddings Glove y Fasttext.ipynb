{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Ua7AcJXvEW"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Word Embeddings con Glove y Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V64c1mLJeQFq"
   },
   "source": [
    "Este código permite levantar los embeddings de fastText y de GloVe y poder utilizarlos para calcular la distancia entre palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U2K2RERweIY",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoahGjfGX1vN"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvaLePMwpBjV",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n",
    "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
    "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
    "# Download fastText Embeddings \n",
    "!curl -L -o 'fasttext.pkl' 'https://drive.google.com/u/0/uc?id=1Qi1r-u5lsEsNqRSxLrpNOqQ3B_ufltCa&export=download&confirm=t'\n",
    "\n",
    "# Download GloVe Embeddings\n",
    "!curl -L -o 'gloveembedding.pkl' 'https://drive.google.com/u/0/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0RVEeHfzbg3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# De donde se obtuvieron estos embeddings:\n",
    "\n",
    "# Link de donde se dscargo los embeddings de GloVe (de ese zip solo se sube el embedding de 50d)\n",
    "# http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# Link de descarga de los embeddings 300 de fastText\n",
    "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCuwnSJn7WPF",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Paper de GloVe\n",
    "# https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTNWSWVWaBno"
   },
   "source": [
    "### 1 - Ensayar los embeddings de Glove y Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9HDjEUNd4ff",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Acá definimos una clase que nos va a permitir manejar los embeddings de GloVe\n",
    "# y fastText con la misma interfase.\n",
    "# Incluye cómo cargar los embeddings a partir de sus pickles \n",
    "# (formato de datos serializados de Python) y guardarlos\n",
    "# funciones para obtener términos dados índices y viceversa\n",
    "\n",
    "class WordsEmbeddings(object):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        # load the embeddings\n",
    "        words_embedding_pkl = Path(self.PKL_PATH)\n",
    "        if not words_embedding_pkl.is_file():\n",
    "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
    "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
    "            embeddings = self.convert_model_to_pickle()\n",
    "        else:\n",
    "            embeddings = self.load_model_from_pickle()\n",
    "        self.embeddings = embeddings\n",
    "        # build the vocabulary hashmap\n",
    "        index = np.arange(self.embeddings.shape[0])\n",
    "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
    "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
    "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
    "\n",
    "    def get_words_embeddings(self, words):\n",
    "        words_idxs = self.words2idxs(words)\n",
    "        return self.embeddings[words_idxs]['embedding']\n",
    "\n",
    "    def words2idxs(self, words):\n",
    "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
    "\n",
    "    def idxs2words(self, idxs):\n",
    "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
    "\n",
    "    def load_model_from_pickle(self):\n",
    "        self.logger.debug(\n",
    "            'loading words embeddings from pickle {}'.format(\n",
    "                self.PKL_PATH\n",
    "            )\n",
    "        )\n",
    "        max_bytes = 2**28 - 1 # 256MB\n",
    "        bytes_in = bytearray(0)\n",
    "        input_size = os.path.getsize(self.PKL_PATH)\n",
    "        with open(self.PKL_PATH, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        embeddings = pickle.loads(bytes_in)\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "    def convert_model_to_pickle(self):\n",
    "        # create a numpy strctured array:\n",
    "        # word     embedding\n",
    "        # U50      np.float32[]\n",
    "        # word_1   a, b, c\n",
    "        # word_2   d, e, f\n",
    "        # ...\n",
    "        # word_n   g, h, i\n",
    "        self.logger.debug(\n",
    "            'converting and loading words embeddings from text file {}'.format(\n",
    "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
    "            )\n",
    "        )\n",
    "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
    "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
    "        structure = np.dtype(structure)\n",
    "        # load numpy array from disk using a generator\n",
    "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
    "            embeddings_gen = (\n",
    "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
    "                if len(line.split()[1:]) == self.N_FEATURES\n",
    "            )\n",
    "            embeddings = np.fromiter(embeddings_gen, structure)\n",
    "        # add a null embedding\n",
    "        null_embedding = np.array(\n",
    "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
    "            dtype=structure\n",
    "        )\n",
    "        embeddings = np.concatenate([embeddings, null_embedding])\n",
    "        # dump numpy array to disk using pickle\n",
    "        max_bytes = 2**28 - 1 # # 256MB\n",
    "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(self.PKL_PATH, 'wb') as f_out:\n",
    "            for idx in range(0, len(bytes_out), max_bytes):\n",
    "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "# Armamos clases particulares para manejar los embeddings de Glove y Fasttext\n",
    "# que heredan de la clase anterior WordsEmbeddings\n",
    "class GloveEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
    "    PKL_PATH = 'gloveembedding.pkl'\n",
    "    N_FEATURES = 50\n",
    "    WORD_MAX_SIZE = 60\n",
    "\n",
    "\n",
    "class FasttextEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
    "    PKL_PATH = 'fasttext.pkl'\n",
    "    N_FEATURES = 300\n",
    "    WORD_MAX_SIZE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiCtPUYirc1t",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos los generadores de embeddings\n",
    "# Puede demorar 1 minuto porque tiene que levantar los archivos de embeddings\n",
    "model_fasttext = FasttextEmbeddings()\n",
    "model_glove = GloveEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQKIDrCWfuxV",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Armamos una función para graficar la matriz de similaridad\n",
    "def plot_matrix_distance(words, dist):\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_subplot()\n",
    "    sns.heatmap(dist, xticklabels=words, yticklabels=words, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCBmo0npei9x",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Ojo que si usan scipy la distancia coseno no es la similitud coseno (esta \"negada\")\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "def embeddings_matrix_distance(model, words):\n",
    "    print(\"Cantidad de palabras:\", len(words))\n",
    "    emb = model.get_words_embeddings(words)\n",
    "    print(\"Dimensiones de los embeddings:\", emb.shape)\n",
    "    dist = pairwise.cosine_similarity(emb, emb)\n",
    "    plot_matrix_distance(words, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MlZWL1KaQq2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Palabras a ensayar\n",
    "# Algunas relativas con saludos y otras con dispositivos\n",
    "words = [\"hi\", \"hello\", \"bye\", \"goodbye\", \"morning\", \"computer\", \"machine\", \"laptop\", \"device\", \"printer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnrchHWMZ5-p",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_matrix_distance(model_glove, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hfg7_QxaYBf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_matrix_distance(model_fasttext, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOxqbPg7b52U"
   },
   "source": [
    "### 2 - Conclusión hasta el momento\n",
    "Como era esperado no es igual el resultado, y cada uno tiene sus ventajas según lo que se desea lograr.\n",
    "- En los embeddings de Glove morning está muy relacionado con los saludos y este comportamiento podría no ser deseado.\n",
    "- En los embeddings de Fasttext la similitud entre los dos grupos de palabras elegidas no es tan alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaEtadJHcVMF"
   },
   "source": [
    "### 3 - Operaciones con embeddings (reyes y reinas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VQk0TK1dv7g",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_words = ['king', 'man', 'woman', 'queen']\n",
    "test_emb = model_fasttext.get_words_embeddings(test_words)\n",
    "\n",
    "# king - main + woman\n",
    "new_queen = test_emb[0] - test_emb[1] + test_emb[2]\n",
    "\n",
    "new_words = test_words + ['new_queen']\n",
    "new_emb = np.append(test_emb, new_queen.reshape(1, -1), axis=0)\n",
    "\n",
    "dist = pairwise.cosine_similarity(new_emb, new_emb)\n",
    "\n",
    "plot_matrix_distance(new_words, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkQ7FiblgssY"
   },
   "source": [
    "### 5 - Conclusiones\n",
    "Se puede observar que la similitud entre \"queen\" y \"new_queen\" no es tan alta como uno esperaría y esto depende del generador de embeddings, pero:\n",
    "- Está más cerca \"new_queen\" a \"queen\" de lo que está \"woman\" o \"man\"\n",
    "- Es cierto que \"king\" está más cerca de \"queen\" que \"new_queen\" pero es mutuo.\n",
    "- Están bastante cerca \"man\" y \"woman\" y es por eso que la operación realizada tiene sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKNIdgzujnHS"
   },
   "source": [
    "### 6 - Jugando con otras operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UyJVeokhfDh",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_words2 = ['anarchy', 'monarchy', 'kingdom', 'crown', 'royal']\n",
    "test_emb2 = model_fasttext.get_words_embeddings(test_words2)\n",
    "\n",
    "# king - man\n",
    "no_king = test_emb[0] - test_emb[1]\n",
    "\n",
    "new_words2 = test_words2 + ['no_king']\n",
    "new_emb2 = np.append(test_emb2, no_king.reshape(1, -1), axis=0)\n",
    "\n",
    "dist2 = pairwise.cosine_similarity(new_emb2, new_emb2)\n",
    "\n",
    "plot_matrix_distance(new_words2, dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGsZwv63i4ey",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# king - royal\n",
    "new_man = test_emb[0] - test_emb2[4]\n",
    "\n",
    "new_words3 = test_words + ['new_man']\n",
    "new_emb3 = np.append(test_emb, new_man.reshape(1, -1), axis=0)\n",
    "\n",
    "dist3 = pairwise.cosine_similarity(new_emb3, new_emb3)\n",
    "\n",
    "plot_matrix_distance(new_words3, dist3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvV6FXiehWCV"
   },
   "source": [
    "### 7 - Interpretación gráfica de reyes y reinas\n",
    "Utilizaramos TSNE para representar los embeddings y dos dimensiones y poder compararlos. En una de la mejores formas de reducir la dimensionalidad de embeddings para comparación visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO_UrKi7lWi7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpXLW2M9kG50",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar los primeros embeddings calculados, con \"queen\" y \"new_queen\"\n",
    "X = np.asanyarray(new_emb) # son los embeddings\n",
    "y = np.asanyarray(new_words) # son los strings\n",
    "\n",
    "# Si bien se puede incluir X e y en el método fit_transform de TSNE, y es ignorado\n",
    "# ya que se transforman sólo los datos en X\n",
    "# el parámetro de perplexity debe ser menor a la cantidad de datos a transformar\n",
    "X_embedded = TSNE(n_components=2,perplexity=5.0).fit_transform(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot()\n",
    "sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y, palette='bright', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pXYcXfJkhOE",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar todos los embeddings calculados, con \"new_queen\" y \"no_king\"\n",
    "X = np.asanyarray(np.append(new_emb, new_emb2, axis=0))\n",
    "y = np.asanyarray(new_words + new_words2)\n",
    "\n",
    "X_embedded = TSNE(n_components=2,perplexity=5.0).fit_transform(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot()\n",
    "sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y, palette='bright', ax=ax)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
